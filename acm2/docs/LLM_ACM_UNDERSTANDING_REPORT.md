# LLM Report: Understanding ACM + Eval Code


The following is about  ACM 1.0.


This report was generated by an LLM after reading the source files as instructed by `LLM_READING_GUIDE_FOR_ACM.md`.

---

## 1. What the software does in one short sentence

ACM is a pipeline that generates multiple AI-written reports from input documents using different LLM providers/models (FPF, GPT-Researcher, Multi-Agent), then evaluates and ranks those reports via single-document grading and pairwise comparisons to identify the best output.

---

## 2. What the software does in ten detailed sentences

1. **Input Discovery**: `runner.py` discovers markdown files in a configured `input_folder` and iterates over each to produce outputs; an optional `one_file_only` flag limits processing to a single document.

2. **Generation Phase**: For each input document, the runner spawns multiple report-generation engines—FPF (FilePromptForge), GPT-Researcher (standard and deep modes), and Multi-Agent—each configured via `config.yaml` and `presets.yaml` with provider/model, token limits, and concurrency settings.

3. **FPF Integration**: `fpf_runner.py` invokes `FilePromptForge/fpf_main.py` as a subprocess, passing instructions (file_a) and the input document (file_b); it writes the output to a deterministic path, handles retries for validation failures (missing grounding or reasoning), and returns `(path, model)` tuples.

4. **GPT-Researcher Integration**: `gptr_subprocess.py` runs GPT-Researcher in a child process with environment-injected model settings (`SMART_LLM`, `FAST_LLM`); outputs are captured via JSON on stdout, parsed, and returned to the runner.

5. **Artifact Saving**: `save_generated_reports()` copies generated files into `output_folder`, mirroring the input directory structure and applying a naming convention `{base}.{type}.{iter}.{model}.{uid}.{ext}` to distinguish runs; it also triggers streaming evaluation callbacks if enabled.

6. **Skip Logic**: Before processing, `file_manager.output_exists()` checks whether any file matching the base name already exists in the output directory—if so, the document is skipped; the evaluation phase similarly uses explicit file lists rather than directory scans to avoid re-running work.

7. **Single-Document Evaluation**: `evaluate.py` (and `llm-doc-eval/api.py`) submits each generated report to configured judge models (e.g., Gemini, GPT) via FPF batch mode, parsing JSON responses for per-criterion scores (1-5) and storing them in a per-run SQLite database.

8. **Pairwise Evaluation & Elo**: All report pairs are compared by judge models; winners are recorded in `pairwise_results`, and an Elo rating is computed to identify the overall best report; `get_best_report_by_elo()` returns the top-ranked document path.

9. **Combine & Playoffs**: After initial evaluation, the top 2 reports are passed to a "combiner" that merges their insights into combined reports; these challengers then compete in a "playoffs" round against the original parents using the same pairwise evaluation, with cached scores reused via `source_db` optimizations.

10. **Unified HTML Report**: After playoffs, the runner calls `generate_unified_html_report()` to produce a comprehensive HTML dashboard that includes generation timeline, pre-combiner and playoffs evaluation timelines, cost data parsed from FPF logs, and hyperlinks to all artifacts.

---

## 3. Why someone would use this software

A user would use ACM to **automate and scale comparative LLM research**: instead of manually calling multiple AI providers and eyeballing which output is best, ACM handles the entire workflow—multiple generation engines, multiple iterations, multi-model judging, and statistical ranking—in a single orchestrated run. This saves significant time, reduces human bias in evaluation, and produces a rich audit trail (SQLite DB, CSVs, HTML reports) that can be reviewed, reproduced, or extended. It is particularly valuable when quality matters more than speed—for example, generating policy analyses, research summaries, or legal document drafts—where picking the best of many AI attempts yields noticeably better results than a single shot.

---

## 4. Main phases of a run and how they connect

| Phase | Key Functions / Files | Inputs | Outputs |
|-------|----------------------|--------|---------|
| **1. Config Load** | `config_parser.load_config()` | `config.yaml`, `presets.yaml` | Resolved paths, run list, concurrency settings |
| **2. File Discovery** | `file_manager.find_markdown_files()` | `input_folder` | List of `.md` paths |
| **3. Skip Check** | `file_manager.output_exists()` + prefix-based match | `output_folder` contents | Decision to skip or proceed |
| **4. Generation** | `process_file_run()`, `fpf_runner`, `gptr_subprocess`, `MA_runner` | Query prompt built from input + instructions | Temp files in `TEMP_BASE`, then copied to `output_folder` with standardized names |
| **5. Artifact Save** | `save_generated_reports()` | Generated paths dict | Final artifacts in `output_folder`; streaming eval triggered |
| **6. Single Eval** | `run_single_evaluation()` in `api.py` | Candidate folder, judge models | `single_doc_results` rows in SQLite |
| **7. Pairwise Eval** | `run_pairwise_evaluation()` | Same DB, all pairs | `pairwise_results` rows, Elo ratings |
| **8. Combine** | `ReportCombiner.combine()` | Top 2 reports, instructions | Combined reports in `output_folder` |
| **9. Playoffs** | `trigger_evaluation_for_all_files()` with `is_combined_run=True` | Parent + combined files | Updated DB, winner copied to `winners_dir` |
| **10. HTML Export** | `generate_unified_html_report()` | DBs, timeline JSONs, cost logs | `unified_report.html` in exports dir |

---

## 5. How documents, models, FPF, and evaluation are wired together today

- **Documents** are discovered by `file_manager` from `input_folder` and passed as paths throughout the pipeline.
- **Models** are specified in `config.yaml` under `runs` (for generation) and `eval.judges` (for evaluation). Each run entry has `type` (fpf/gptr/dr/ma), `provider`, and `model`.
- **FPF** is called via `fpf_runner.run_filepromptforge_runs()` or `run_filepromptforge_batch()`, which spawns `fpf_main.py` as a subprocess. Environment variables (`FPF_RUN_GROUP_ID`, `FPF_LOG_DIR`) route logs to per-run folders for cost aggregation.
- **Evaluation** uses FPF as the LLM backend too—`llm-doc-eval/api.py` builds a batch of judge requests and calls `fpf_runner.run_filepromptforge_batch()`, parsing JSON responses for scores.
- **Data flow**: Input doc → query prompt → generation subprocess → artifact file → eval subprocess → SQLite rows → Elo computation → HTML.

---

## 6. Strongest, battle-tested parts (wrap and reuse)

| Component | Strength |
|-----------|----------|
| **FPF (`fpf_runner`, `fpf_main`)** | Robust subprocess invocation with 4-layer retry (exit codes, failure reports, backoff, prompt enhancement). Heavy logging to per-run JSON files with cost tracking. Works reliably even for Google/OpenAI edge cases. |
| **Evaluation DB schema** | Simple but effective (`single_doc_results`, `pairwise_results`). Allows incremental appends, easy CSV export, and Elo calculation without external dependencies. |
| **HTML reporting (`html_exporter`)** | Generates self-contained dashboards with timelines, charts, and hyperlinks—useful for auditing and sharing results. |
| **Skip logic (pattern-based)** | Checking for `{base_name}.*` files in output folder is deterministic and fast; avoids re-running expensive generation or eval. |

---

## 7. Fragile parts (rely on directory scans, magic names, or global state)

| Issue | Location | Risk |
|-------|----------|------|
| **"Most recent export dir" heuristic** | `runner.py` lines ~1420–1450: `eval_dirs = sorted([...], reverse=True)` | If multiple runs overlap or dirs are renamed, wrong pre-combiner data may be used for unified HTML. |
| **Global `DOC_PATHS` dict** | `llm-doc-eval/api.py` | Mutated in-place during evaluation; if two evals run concurrently in the same process, paths can collide. |
| **Implicit `TEMP_BASE` sharing** | `TEMP_BASE` reused across FPF, GPT-R | Concurrent file runs can overwrite each other's temp files. |
| **Env-var injection for subprocess model** | `SMART_LLM`, `FAST_LLM` in `process_file_run()` | Works for isolation but is invisible to debugging; easy to forget to unset. |
| **Timestamp-based log discovery for cost aggregation** | `_aggregate_fpf_costs()` scans `FilePromptForge/logs/<run_group_id>` | If logs are not written (FPF error) or run_group_id is lost, cost data is missing or wrong. |
| **GUI full-path fields** | `GUI/gui.py` stores absolute paths | Breaks portability; ACM 2.0 should use IDs or relative refs. |

---

## 8. Information needed in ACM 2.0 database/API to avoid fragile heuristics

1. **Run ID**: Unique identifier linking all artifacts, logs, and DB rows for one orchestration invocation.
2. **Document ID**: Stable reference to an input document (hash or path within repo) that survives renames.
3. **Artifact manifest**: Explicit list of generated file paths per run, stored in DB, so no directory scanning is needed.
4. **Eval phase tag**: `precombine` vs `postcombine` stored with each eval row so HTML generator can filter without inferring from timestamps.
5. **Cost ledger**: Per-artifact cost recorded at generation time, not reconstructed later from logs.
6. **Status enum**: `pending | running | success | failed` per artifact so skip logic can query DB instead of filesystem.
7. **Storage provider ref**: Abstract path (e.g., `github://owner/repo/path` or `local:///abs/path`) so the system can switch backends without changing run logic.

---

## 9. Summary of key files read

| File | Purpose |
|------|---------|
| `runner.py` | Central orchestration: discovers files, spawns generation runs, triggers evaluation, combine, playoffs, and HTML export. |
| `generate.py` | Simpler entrypoint that delegates to `runner.run()`. |
| `evaluate.py` | CLI wrapper that parses args and calls `llm-doc-eval/api.py` functions; handles CSV/HTML export. |
| `functions/fpf_runner.py` | Subprocess wrapper for FPF; builds commands, streams output, handles retries, returns `(path, model)`. |
| `functions/file_manager.py` | `find_markdown_files()`, `output_exists()`, `get_output_path()` utilities. |
| `llm-doc-eval/llm_doc_eval/api.py` | Core evaluation logic: `run_single_evaluation()`, `run_pairwise_evaluation()`, Elo computation, JSON parsing, DB persistence. |
| `config.yaml` | Runtime configuration: folders, runs, concurrency, eval judges, combine settings. |
| `presets.yaml` | Named presets (`low`, `888`, etc.) with token limits, model lists, enable flags. |

---

*Report generated by LLM following `LLM_READING_GUIDE_FOR_ACM.md`.*
