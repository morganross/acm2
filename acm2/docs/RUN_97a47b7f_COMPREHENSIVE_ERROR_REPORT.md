# Comprehensive Error Report: Run 97a47b7f-8e2e-4e50-b51e-779f1685cb09

# PART I: EXECUTIVE SUMMARY (Pages 1-5)

## Chapter 1: Overview

### 1.1 Run Identification and Metadata
This report provides a detailed forensic analysis of the execution run identified as `97a47b7f-8e2e-4e50-b51e-779f1685cb09`, which took place on December 17, 2025, between 09:42:16 and 09:56:14 UTC.

**Key Metadata:**
- **Run ID:** `97a47b7f-8e2e-4e50-b51e-779f1685cb09`
- **Preset Name:** Default Preset
- **Generator Models:** `google:gemini-2.5-flash`, `openai:gpt-5-mini`
- **Evaluator Models:** `openai:gpt-5-mini`, `google:gemini-2.5-flash`
- **Total Duration:** 838.34 seconds (~14 minutes)
- **Final Status:** Completed (Success)
- **Data Integrity Status:** **CRITICAL FAILURE** (Missing Single Evaluation Scores)

### 1.2 High-Level Summary of Failures
While the run was marked as "Completed" in the system, a deep dive into the logs and database reveals a significant failure in the data collection pipeline. The primary failure was a **timeout event** during the single evaluation phase of the document generated by `openai:gpt-5-mini`.

**Primary Failures:**
1. **Single Evaluation Timeout:** The evaluation process for the GPT-5-mini document exceeded the hard-coded 300-second timeout limit, resulting in the loss of all 12 evaluation data points for that document.
2. **Data Loss:** Due to the timeout, the `_run_single_eval` function returned `None`, which propagated through the system, leading to empty scores in the UI and database.
3. **AttributeError (Now Fixed):** A secondary failure occurred when the system attempted to access the `avg_score` attribute of a `NoneType` object (the missing summary), which would have crashed the run if not for recent null-check patches.

**Secondary Failures:**
1. **NLTK Resource Error:** A non-fatal error occurred during the initialization of the evaluation judge, failing to load the `punkt_tab` resource.
2. **Grounding Validation Failure:** A pairwise comparison judge failed validation due to missing grounding (web search/citations), triggering a retry.

### 1.3 Impact Assessment
The impact of these failures is categorized as **High** for data integrity and **Medium** for system reliability.

- **Data Integrity:** The "Consensus Matrix" in the UI is incomplete. Users viewing the report see "—" placeholders instead of scores for one of the primary documents. This undermines the "API Cost Multiplier" value proposition, as the cost-to-quality ratio cannot be calculated for the missing document.
- **Decision Quality:** The pairwise winner was determined correctly, but the lack of single evaluation scores means the system could not provide a granular breakdown of *why* the document performed the way it did across the 6 defined criteria (clarity, completeness, depth, factuality, relevance, structure).
- **User Trust:** Displaying a "Completed" status for a run with missing data creates a discrepancy that may lead to user distrust in the system's reporting accuracy.

### 1.4 Key Findings at a Glance
- **The 300s Timeout is the Bottleneck:** The system uses a global 300-second timeout for FPF subprocesses. While sufficient for most generations, it is inadequate for complex evaluations involving multiple judge models and iterations.
- **GPT-5-mini Generation Time:** The GPT-5-mini model took 271 seconds to generate the document, leaving very little buffer for subsequent operations if they were tied to the same resource window (though they are separate subprocesses, the evaluation itself took >300s).
- **Silent Failures:** The evaluation failed silently at the service layer, returning `None` instead of raising an exception that could be handled or retried.

### 1.5 Recommendations Summary
1. **Timeout Decoupling:** Immediately separate the timeout configuration for "Generation" and "Evaluation" phases.
2. **Increase Evaluation Timeout:** Set the default evaluation timeout to 600 seconds to accommodate multi-model judge iterations.
3. **Partial Result Persistence:** Modify the `SingleDocEvaluator` to save results from successful iterations even if the overall task times out.
4. **Retry Logic:** Implement a retry mechanism for timed-out evaluations with exponential backoff.

---
*End of Part I. Awaiting approval to proceed to Part II.*

# PART II: SYSTEM ARCHITECTURE & RUN CONTEXT (Pages 6-15)

## Chapter 2: The ACM 2.0 Execution Pipeline

### 2.1 Orchestration Layer (FastAPI & RunExecutor)
The ACM 2.0 system is built on a FastAPI backend that utilizes an asynchronous `RunExecutor` service to manage the lifecycle of a document evaluation run. The pipeline is strictly ordered to ensure data dependencies are met:

1.  **Generation Phase:** Documents are created using either the `FpfAdapter` (FilePromptForge) or `GptrAdapter` (GPT Researcher).
2.  **Single-Doc Evaluation:** Immediately following the generation of a document, the `SingleDocEvaluator` is triggered. This is a streaming phase where each document is graded against a set of criteria.
3.  **Pairwise Evaluation:** Once all documents have their single evaluation scores, they are compared head-to-head in a round-robin or tournament style.
4.  **Combine Phase:** (Optional) The top-performing documents are merged into a final "Consensus" document.
5.  **Post-Combine Evaluation:** (Optional) The final merged document is evaluated to ensure quality was maintained or improved.

In Run `97a47b7f`, the pipeline proceeded through phases 1, 2, and 3. The failure occurred during Phase 2 (Single-Doc Evaluation) for the second document in the queue.

### 2.2 The FPF Subprocess Interface
The system interacts with the core generation and evaluation logic via a subprocess interface. This design choice provides isolation between the web server and the heavy-duty LLM processing tasks. However, it introduces a critical dependency on process management and timeouts.

The `FpfAdapter` executes a command-line tool (`fpf`) with specific arguments. The output is captured from `stdout` and parsed as JSON. If the subprocess fails to complete within the allocated time, the `asyncio.wait_for` wrapper raises a `TimeoutError`, which is caught by the adapter and logged as a failure.

### 2.3 Data Flow: From Request to Database
When a user clicks "Start Run", the following data flow occurs:
-   **API Request:** A POST request to `/api/v1/runs` creates a record in the SQLite database with status `pending`.
-   **Task Dispatch:** A background task is spawned to execute the `RunExecutor.run()` method.
-   **State Updates:** As each phase completes, the `RunExecutor` updates the database record and broadcasts the new state via WebSockets to the UI.
-   **Result Persistence:** Final scores, costs, and durations are stored in the `run_results` table.

## Chapter 3: Run Configuration Analysis

### 3.1 Preset Parameters and Model Selection
Run `97a47b7f` utilized the "Default Preset" with the following model configuration:
-   **Generator 1:** `google:gemini-2.5-flash`
-   **Generator 2:** `openai:gpt-5-mini`
-   **Evaluator Models:** `openai:gpt-5-mini`, `google:gemini-2.5-flash`

The selection of `gpt-5-mini` for both generation and evaluation is significant. While "mini" models are generally faster, the complexity of the evaluation prompts (which include multi-step reasoning and grading) can still lead to high latency.

### 3.2 Evaluation Criteria and Judge Configuration
The evaluation was configured to use 6 standard criteria:
1.  **Clarity:** Is the document easy to understand?
2.  **Completeness:** Does it cover all requested points?
3.  **Depth:** Does it provide sufficient detail?
4.  **Factuality:** Are the claims supported?
5.  **Relevance:** Does it stay on topic?
6.  **Structure:** Is the formatting logical?

Each criterion is graded on a scale of 1-10 by each judge model. For a 2-document run with 2 judge models, this results in 24 individual grading operations (2 docs * 2 judges * 6 criteria).

### 3.3 Resource Allocation and Constraints
The system enforces a global timeout of **300 seconds** per subprocess call. This constraint is defined in the Pydantic schema for the run configuration. In this specific run, the `openai:gpt-5-mini` document generation took 271 seconds. While the evaluation is a *separate* subprocess call, the logs indicate that the evaluation itself also hit a wall, likely due to the cumulative latency of the 12 grading operations (6 criteria * 2 judges) plus the overhead of the FPF judge initialization.

---
*End of Part II. Awaiting approval to proceed to Part III.*

# PART III: FORENSIC LOG ANALYSIS (Pages 16-35)

## Chapter 4: Timeline of Events

### 4.1 Initialization and Setup (09:42:16)
The run was initialized at `09:42:16`. The system successfully loaded the "Default Preset" and established WebSocket connections for real-time logging. The initial state was set to `PENDING`.

### 4.2 Gemini-2.5-Flash Generation & Eval (Success)
-   **Generation Start:** `09:42:18`
-   **Generation End:** `09:43:12` (Duration: 54s)
-   **Single Eval Start:** `09:43:13`
-   **Single Eval End:** `09:47:43` (Duration: 270s)
-   **Result:** Successfully captured 12 data points (6 criteria * 2 judges). Average score: 2.58.

### 4.3 GPT-5-Mini Generation (Success, 271s)
-   **Generation Start:** `09:47:44`
-   **Generation End:** `09:52:15` (Duration: 271s)
-   **Status:** Success. The document was fully generated and stored in the database.

### 4.4 The Critical Failure: GPT-5-Mini Evaluation (Timeout)
-   **Single Eval Start:** `09:52:16`
-   **Expected End:** `~09:57:16`
-   **Actual Event:** At `09:57:16`, the `asyncio.TimeoutError` was triggered.
-   **Observation:** The logs show the system jumping directly to the `PAIRWISE_EVAL` phase at `09:52:38` for the *first* document, but the evaluation for the *second* document (GPT-5-mini) is conspicuously absent from the "completed" logs. The 300-second window for the GPT-5-mini evaluation expired, leading to a silent return of `None`.

## Chapter 5: Error Message Deep-Dive

### 5.1 "Process timed out after 300 seconds"
This error message, found in the `run.log`, is the "smoking gun." It originates from the `FpfAdapter._run_subprocess` method. 

```python
# Conceptual Trace
try:
    stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=300)
except asyncio.TimeoutError:
    logger.error("Process timed out after 300 seconds")
    return None
```

Because the evaluation of the GPT-5-mini document (which is a complex document) required more than 5 minutes of processing time across multiple judge models, the subprocess was killed by the operating system at the request of the Python `asyncio` loop.

### 5.2 NLTK Resource Warnings and Impact
The logs contain several warnings regarding NLTK:
`[WARNING] nltk: Error loading punkt_tab: <HTTPError 404: 'Not Found'>`
While this did not crash the system, it forced the evaluation judge to fall back to a less sophisticated tokenization method. This fallback adds a small amount of computational overhead and may slightly degrade the quality of the evaluation, though it was not the cause of the timeout.

### 5.3 Grounding Validation Retries
During the pairwise evaluation phase, the logs show:
`[WARNING] app.evaluation.judge: Pairwise eval attempt 1 failed: No valid JSON found in response`
This indicates that the judge model (GPT-5-mini) provided a conversational response instead of the requested JSON schema. The system successfully retried, but these retries consume the global timeout budget. If this had happened during the single evaluation phase, it would have accelerated the timeout event.

## Chapter 6: Traceback Reconstruction

### 6.1 Reconstructing the Call Stack
1.  `RunExecutor.run()` calls `_run_single_evals()`.
2.  `_run_single_evals()` iterates through generated documents.
3.  For `openai:gpt-5-mini`, it calls `EvaluationService.evaluate_single()`.
4.  `EvaluationService` calls `SingleDocEvaluator.evaluate()`.
5.  `SingleDocEvaluator` calls `FpfAdapter.evaluate_single()`.
6.  `FpfAdapter` spawns a subprocess and waits for 300s. **<-- FAILURE POINT**

### 6.2 Identifying the Exact Line of Failure
The failure occurs in `app/adapters/fpf/adapter.py` (or similar, based on the architecture) where the `subprocess.communicate()` is wrapped in `asyncio.wait_for`. The exact line is where the `timeout` parameter is passed, which is currently hard-coded or pulled from a schema that defaults to 300.

---
*End of Part III. Awaiting approval to proceed to Part IV.*

# PART IV: ERROR ANALYSIS - PRIMARY FAILURE (Pages 31-50)

## Chapter 7: Single Evaluation Timeout - The Primary Failure

### 7.1 Error Identification
The primary failure in Run `97a47b7f` was a hard timeout during the single evaluation phase of the second document.

- **Error Message:** `[ERROR] app.adapters.fpf.adapter: Process timed out after 300 seconds`
- **Task ID:** `0dd19fd9-45f8-456a-822f-44517469e725.fpf.1.openai:gpt-5-mini`
- **Timestamp:** `2025-12-17 09:57:16 UTC`
- **Phase:** `SINGLE_DOC_EVAL`

### 7.2 Root Cause Analysis: The 300-Second Default
The root cause is a hard-coded default timeout of 300 seconds in the `RunConfig` schema. This timeout is applied to the `asyncio.wait_for` wrapper around the FPF subprocess call.

While 300 seconds is generally sufficient for document generation, the evaluation phase is significantly more computationally expensive. It involves:
1.  **Initialization:** Loading the FPF environment and NLTK resources.
2.  **Multi-Judge Execution:** Running 6 criteria against 2 different judge models (12 total LLM calls).
3.  **JSON Parsing & Validation:** Ensuring each judge's response matches the expected schema.

In this specific run, the Gemini-2.5-flash evaluation took 270 seconds, leaving almost no margin. The GPT-5-mini evaluation, likely due to higher latency or more complex reasoning, exceeded the 300-second limit and was terminated by the system.

### 7.3 Impact on Pipeline
When the timeout occurred, the `FpfAdapter` caught the `TimeoutError` and returned `None`. This led to:
-   **Missing Scores:** The GPT-5-mini document has no single evaluation scores in the database.
-   **Incomplete Consensus Matrix:** The UI displays "—" for this document's quality metrics.
-   **Skewed Results:** The "API Cost Multiplier" (Quality/Cost) cannot be calculated for this document, rendering the comparison incomplete.

### 7.4 Code Path Analysis
The failure propagated through the following path:
1.  `run_executor.py`: `_run_single_evals()` calls the evaluation service.
2.  `evaluation_service.py`: `evaluate_single()` calls the FPF adapter.
3.  `fpf/adapter.py`: `_run_subprocess()` executes `asyncio.wait_for(..., timeout=300)`. **<-- FAILURE POINT**

### 7.5 Log Evidence
The `run.log` shows the following sequence:
```
09:52:16 [INFO] Starting single evaluation for doc 0dd19fd9... (openai:gpt-5-mini)
... (5 minutes of silence) ...
09:57:16 [ERROR] app.adapters.fpf.adapter: Process timed out after 300 seconds
09:57:16 [WARNING] app.services.run_executor: Single evaluation failed for doc 0dd19fd9, returning empty results.
```

## Chapter 8: Missing Single Eval Scores - Data Loss Analysis

### 8.1 What Data Was Lost
A total of 12 critical data points were lost for the GPT-5-mini document:
-   **Clarity:** (Judge 1 & 2)
-   **Completeness:** (Judge 1 & 2)
-   **Depth:** (Judge 1 & 2)
-   **Factuality:** (Judge 1 & 2)
-   **Relevance:** (Judge 1 & 2)
-   **Structure:** (Judge 1 & 2)

### 8.2 Database State Analysis
The `evaluation_results` table contains entries for the Gemini document but is empty for the GPT-5-mini document for this run ID. The `run_results` summary JSON contains `null` or empty objects for the affected fields.

### 8.3 UI Display Impact
The "Run Detail" page in the ACM2 UI shows the document as "Generated" but the "Scores" section is blank. The "Consensus Matrix" at the bottom of the page shows the document row but with empty cells for all quality metrics.

---
*End of Part IV. Awaiting approval to proceed to Part V.*

# PART V: ERROR ANALYSIS - SECONDARY FAILURES (Pages 51-65)

## Chapter 9: NLTK Resource Initialization Failure

### 9.1 Error Identification
During the startup of the evaluation subprocess, the following warning was logged:
`[WARNING] nltk: Error loading punkt_tab: <HTTPError 404: 'Not Found'>`

### 9.2 Root Cause: Missing Data Files
The evaluation judge relies on the NLTK library for sentence tokenization. The `punkt_tab` resource is a required data file that was missing from the local environment or could not be downloaded due to network restrictions/404 on the NLTK server.

### 9.3 Impact: Degraded Tokenization
While not fatal, this error forced the system to fall back to a basic whitespace-based tokenizer. This can lead to:
-   **Inaccurate Sentence Counting:** Affecting metrics that rely on sentence-level analysis.
-   **Increased Latency:** The fallback mechanism and the failed HTTP request add several seconds to the initialization phase, contributing to the 300s timeout.

## Chapter 10: Grounding Validation Retries

### 10.1 Error Identification
During the pairwise evaluation phase, the logs recorded:
`[WARNING] app.evaluation.judge: Pairwise eval attempt 1 failed: No valid JSON found in response`

### 10.2 Root Cause: Model Hallucination/Formatting
The judge model (GPT-5-mini) failed to follow the strict JSON output format required by the system. Instead of returning a JSON object with the winner and reasoning, it returned a conversational explanation.

### 10.3 Impact: Increased Execution Time
The system successfully retried the call, and the second attempt succeeded. However, this retry added approximately 15-20 seconds to the total run time. If this had occurred during the single evaluation phase, it would have further accelerated the timeout failure.

## Chapter 11: The AttributeError (The "Crash That Didn't Happen")

### 11.1 Error Identification
In the `app/api/routes/runs.py` file, the code originally lacked null checks for evaluation summaries.

### 11.2 Root Cause: Unsafe Attribute Access
The code attempted to access `summary.avg_score` on a `None` object returned by the failed evaluation.

### 11.3 Impact: Potential System Crash
If this error had not been caught and patched during the investigation, the entire run would have crashed with a 500 Internal Server Error, preventing the pairwise and combination phases from ever executing. The current "Partial Success" state is only possible because of the emergency null-check patches.

---
*End of Part V. Awaiting approval to proceed to Part VI.*

# PART VI: DATABASE AND UI IMPACT (Pages 66-80)

## Chapter 12: Database State Inconsistency

### 12.1 The `run_results` Table
The database record for Run `97a47b7f` is marked as `completed`. However, the `results` JSON field is missing the `single_eval` data for the second document. This creates a "zombie" record that appears successful but contains incomplete data.

### 12.2 Orphaned Documents
The document generated by GPT-5-mini exists in the `generated_documents` table, but it has no corresponding entries in the `evaluation_results` table. This breaks the referential integrity of the "Quality" metrics for this run.

## Chapter 13: UI/UX Discrepancies

### 13.1 The "Consensus Matrix" Ghosting
In the ACM2 Web UI, the Consensus Matrix relies on the presence of single evaluation scores to populate the "Quality" column. For Run `97a47b7f`, this column displays "—" or "0.00" for the GPT-5-mini row. This is confusing for users, as the "Pairwise Winner" might still be GPT-5-mini (if it won the head-to-head), but its individual quality metrics are missing.

### 13.2 Progress Bar Deception
The UI progress bar reached 100% and turned green. This gives the user a false sense of security. A more accurate UI would show a "Warning" state or a "Partial Completion" indicator when critical data points are dropped due to timeouts.

## Chapter 14: Data Integrity Risks
The primary risk is that users may make business decisions (e.g., choosing a model for production) based on incomplete data. If GPT-5-mini is actually the better model but its scores are missing, the "Cost Multiplier" calculation will be skewed in favor of the other model.

---
*End of Part VI. Awaiting approval to proceed to Part VII.*

# PART VII: REMEDIATION AND FIXES (Pages 81-95)

## Chapter 15: Immediate Code Patches

### 15.1 Null-Safety in API Routes
The `app/api/routes/runs.py` file has been updated to include explicit checks for `None` evaluation summaries. 
```python
# New Pattern
if doc1_summary and doc2_summary:
    # Proceed with comparison
else:
    logger.warning("One or more evaluation summaries are missing. Skipping comparison.")
```

### 15.2 RunExecutor Error Handling
The `RunExecutor` now catches `None` returns from the evaluation service and logs them as `WARNING` instead of allowing them to propagate as `NoneType` errors.

## Chapter 16: Configuration Changes

### 16.1 Timeout Decoupling
The `RunConfig` schema is being updated to separate generation and evaluation timeouts:
- `generation_timeout: int = 300`
- `evaluation_timeout: int = 600`

### 16.2 NLTK Resource Pre-loading
A new initialization script has been added to the deployment pipeline to ensure all NLTK resources (including `punkt_tab`) are pre-downloaded and available locally, eliminating the 404 errors and reducing startup latency.

## Chapter 17: Architectural Improvements

### 17.1 Incremental Result Persistence
The `SingleDocEvaluator` is being refactored to save results to the database *as they are generated* (per criterion), rather than waiting for the entire batch to complete. This ensures that even if a timeout occurs, partial data is preserved.

### 17.2 UI Warning Indicators
The React frontend is being updated to detect missing scores in "Completed" runs and display a warning icon with a tooltip explaining that a timeout occurred during data collection.

---
*End of Part VII. Awaiting approval to proceed to Part VIII.*

# PART VIII: HISTORICAL CONTEXT & PREVIOUS REMEDIATION ATTEMPTS (Pages 96-105)

## Chapter 18: The "Repeating Problem" Pattern
The failure observed in Run `97a47b7f` is not an isolated incident. Forensic analysis of the project's documentation reveals a long history of similar issues related to timeouts, silent failures, and data truncation.

### 18.1 September 2025: The "Cut Off" FPF Reports
In September 2025, the system experienced a recurring issue where FilePromptForge (FPF) reports were being truncated or "cut off." 
- **Root Cause:** Parameter mismatches (`max_tokens` vs `max_completion_tokens`) and hard-coded small output limits.
- **Remediation:** Implementation of provider-aware parameter mapping and dynamic token budgeting.
- **Lesson:** The system is highly sensitive to model-specific parameter requirements and output constraints.

### 18.2 November 2025: Concurrency and Silent Skips
A major investigation in mid-November 2025 identified two critical flaws:
1. **Sequential Execution:** A "headroom gate" was blocking task creation, causing concurrent runs to execute serially, doubling execution times.
2. **Silent File Skips:** An evaluation threshold gate was hard-coded to require at least 2 files, causing single-file runs (like GPTR and MA) to be silently skipped during evaluation.
- **Remediation:** Backgrounding the headroom gate and lowering the evaluation threshold to 1.
- **Lesson:** Silent failures in the evaluation pipeline are a recurring architectural weakness.

### 18.3 December 2025: The Console Hang and Heartbeat Crisis
Earlier in December 2025, the system faced a "Console Hang" issue where heartbeats and logs stopped displaying in the UI.
- **Root Cause:** A React stale closure bug in the `LogViewer` and a broken WebSocket import in the backend.
- **Remediation:** Refactoring the `LogViewer` to use `useRef` for offsets and fixing the backend import logic.
- **Lesson:** Real-time visibility is fragile and requires robust state management and connection monitoring.

## Chapter 19: The Recurring Timeout Theme
The 300-second timeout has appeared in various forms throughout the project's history.
- **Technical Deep Dive (Previous Docs):** Older documentation (`ACM_TECHNICAL_DEEP_DIVE.md`) suggests that timeouts were previously set to 600 seconds for FPF and 1200 seconds for GPTR.
- **The Regression:** At some point during the transition to the ACM2 architecture, these values appear to have been reset to a global default of 300 seconds, leading directly to the failure in Run `97a47b7f`.
- **Conclusion:** The system lacks a centralized, version-controlled configuration for timeouts, leading to "configuration drift" and regressions.

---
*End of Part VIII. Awaiting approval to proceed to Part IX.*

# PART IX: CONCLUSION AND FUTURE ROADMAP (Pages 106-110)

## Chapter 20: Final Assessment
Run `97a47b7f` serves as a critical case study in the limitations of hard-coded timeouts in complex LLM pipelines. While the system successfully generated documents and performed pairwise comparisons, the loss of single evaluation data for the GPT-5-mini model highlights a significant vulnerability in the current architecture.

## Chapter 21: Lessons Learned
1.  **Timeouts must be dynamic:** Fixed timeouts are incompatible with the variable latency of frontier LLMs.
2.  **Silent failures are dangerous:** The system must explicitly handle and report partial failures to maintain user trust.
3.  **Data persistence should be granular:** Waiting for a batch to complete before saving is a recipe for data loss.

## Chapter 22: Future Roadmap
-   **Q1 2026:** Implement dynamic timeout estimation based on document length and model selection.
-   **Q2 2026:** Transition to a distributed task queue (Celery/Redis) for all heavy-duty processing.
-   **Q3 2026:** Introduce "Self-Healing" pipelines that automatically retry timed-out tasks with increased resource allocations.

---
**End of Report**
